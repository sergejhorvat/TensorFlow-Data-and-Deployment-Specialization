{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "legacy_and_s3_api.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b8aa6dc4fa834f21815b822322290893": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3b4f5f956e4f4734889b427c7d3d719a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2fa7657413aa40278a7e0f8d2498d863",
              "IPY_MODEL_e13b23dbc5a245cfb446a8612f97289e"
            ]
          }
        },
        "3b4f5f956e4f4734889b427c7d3d719a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2fa7657413aa40278a7e0f8d2498d863": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dc3e180cb2f5490d81c223489c42ad8e",
            "_dom_classes": [],
            "description": "Dl Completed...: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 13,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 13,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_13aeb656531c4400aba340b1b642a569"
          }
        },
        "e13b23dbc5a245cfb446a8612f97289e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_35dc7f91b42d4f36b79f0fa2fc4f6798",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13/13 [00:11&lt;00:00,  1.18 file/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_be6940a5d8434e4b9a9fca5d0bf1d5c3"
          }
        },
        "dc3e180cb2f5490d81c223489c42ad8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "13aeb656531c4400aba340b1b642a569": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "35dc7f91b42d4f36b79f0fa2fc4f6798": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "be6940a5d8434e4b9a9fca5d0bf1d5c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergejhorvat/TensorFlow-Data-and-Deployment-Specialization/blob/master/Data%20Pipelines%20with%20TensorFlow%20Data%20Services/Week%201/Examples/legacy_and_s3_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX4Kg8DUTKWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaORPzYjQyc7",
        "colab_type": "text"
      },
      "source": [
        "# Legacy and S3 APIs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmQZCg2DQyc-",
        "colab_type": "text"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20Deployment/Course%203%20-%20TensorFlow%20Datasets/Week%201/Examples/legacy_and_s3_api.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/lmoroney/dlaicourse/blob/master/TensorFlow%20Deployment/Course%203%20-%20TensorFlow%20Datasets/Week%201/Examples/legacy_and_s3_api.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApfuRx05QydB",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, we'll take a look at the Legacy and S3 APIs for TensorFlow datasets. We'll explore both the Legacy API which is object-based and the new API which was nicknamed S3 for Split, Slices, and Strings. The new S3 API will eventually become the default API for TensorFlow Datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK48llYkQydD",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "\n",
        "We'll start by importing TensorFlow and TensorFlow Datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV-fE7t2QydE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TTBSvHcSLBzc",
        "outputId": "10a5cbe4-a352-46ba-aaa9-78f9a311d32b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(\"\\u2022 Using TensorFlow Version:\", tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "• Using TensorFlow Version: 2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lxegkF_n4io6"
      },
      "source": [
        "## Merging Splits with the Legacy API\n",
        "\n",
        "In the Legacy API, you can merge splits together by adding them together, as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cb7Qnd0b4c6F",
        "outputId": "71442026-8c11-44a0-c6b2-df7f91139d85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298,
          "referenced_widgets": [
            "b8aa6dc4fa834f21815b822322290893",
            "3b4f5f956e4f4734889b427c7d3d719a",
            "2fa7657413aa40278a7e0f8d2498d863",
            "e13b23dbc5a245cfb446a8612f97289e",
            "dc3e180cb2f5490d81c223489c42ad8e",
            "13aeb656531c4400aba340b1b642a569",
            "35dc7f91b42d4f36b79f0fa2fc4f6798",
            "be6940a5d8434e4b9a9fca5d0bf1d5c3"
          ]
        }
      },
      "source": [
        "all_splits = tfds.Split.TRAIN + tfds.Split.TEST\n",
        "\n",
        "ds = tfds.load(\"mnist:1.0.0\", split=all_splits)\n",
        "\n",
        "print(\"Number of Records: {:,}\".format(len(list(ds))))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found a different version 3.0.0 of dataset mnist in data_dir /root/tensorflow_datasets. Using currently defined version 1.0.0.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset mnist/1.0.0 (download: 11.06 MiB, generated: Unknown size, total: 11.06 MiB) to /root/tensorflow_datasets/mnist/1.0.0...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead set\n",
            "data_dir=gs://tfds-data/datasets.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8aa6dc4fa834f21815b822322290893",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Dl Completed...', max=13, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
            "Number of Records: 70,000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ctqDE_Z95BFq"
      },
      "source": [
        "## Subsplitting with the Legacy API\n",
        "\n",
        "With the Legacy API, we can use the `subsplit` method to divide the datasets. In the example below, we divide the training set into four splits by specifying the number of subsplits, with the argument `k=4`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y3XwKrmF4qCb",
        "outputId": "6379107f-c5b1-474f-9381-5e4e48aa494e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "s1, s2, s3, s4 = tfds.Split.TRAIN.subsplit(k=4)\n",
        "\n",
        "dataset_split_1 = tfds.load(\"mnist:1.0.0\", split=s1)\n",
        "dataset_split_2 = tfds.load(\"mnist:1.0.0\", split=s2)\n",
        "dataset_split_3 = tfds.load(\"mnist:1.0.0\", split=s3)\n",
        "dataset_split_4 = tfds.load(\"mnist:1.0.0\", split=s4)\n",
        "\n",
        "print(len(list(dataset_split_1)))\n",
        "print(len(list(dataset_split_2)))\n",
        "print(len(list(dataset_split_3)))\n",
        "print(len(list(dataset_split_4)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found a different version 3.0.0 of dataset mnist in data_dir /root/tensorflow_datasets. Using currently defined version 1.0.0.\n",
            "WARNING:absl:Found a different version 3.0.0 of dataset mnist in data_dir /root/tensorflow_datasets. Using currently defined version 1.0.0.\n",
            "WARNING:absl:Found a different version 3.0.0 of dataset mnist in data_dir /root/tensorflow_datasets. Using currently defined version 1.0.0.\n",
            "WARNING:absl:Found a different version 3.0.0 of dataset mnist in data_dir /root/tensorflow_datasets. Using currently defined version 1.0.0.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "15000\n",
            "15000\n",
            "15000\n",
            "15000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22GFpItlQydf",
        "colab_type": "text"
      },
      "source": [
        "We can also perform the same operation, by specifying a percentage slice in the `subsplit` method instead. In the example below, we divide the training set into four splits by specifying a percentage slice, with `tfds.percent`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cHeH3eHB5GNZ",
        "outputId": "0c953bb7-1c6f-4837-cbc8-b993167dd73a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "s1 = tfds.Split.TRAIN.subsplit(tfds.percent[0:25])\n",
        "s2 = tfds.Split.TRAIN.subsplit(tfds.percent[25:50])\n",
        "s3 = tfds.Split.TRAIN.subsplit(tfds.percent[50:75])\n",
        "s4 = tfds.Split.TRAIN.subsplit(tfds.percent[75:100])\n",
        "\n",
        "dataset_split_1 = tfds.load(\"mnist:1.0.0\", split=s1)\n",
        "dataset_split_2 = tfds.load(\"mnist:1.0.0\", split=s2)\n",
        "dataset_split_3 = tfds.load(\"mnist:1.0.0\", split=s3)\n",
        "dataset_split_4 = tfds.load(\"mnist:1.0.0\", split=s4)\n",
        "\n",
        "print(len(list(dataset_split_1)))\n",
        "print(len(list(dataset_split_2)))\n",
        "print(len(list(dataset_split_3)))\n",
        "print(len(list(dataset_split_4)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found a different version 3.0.0 of dataset mnist in data_dir /root/tensorflow_datasets. Using currently defined version 1.0.0.\n",
            "WARNING:absl:Found a different version 3.0.0 of dataset mnist in data_dir /root/tensorflow_datasets. Using currently defined version 1.0.0.\n",
            "WARNING:absl:Found a different version 3.0.0 of dataset mnist in data_dir /root/tensorflow_datasets. Using currently defined version 1.0.0.\n",
            "WARNING:absl:Found a different version 3.0.0 of dataset mnist in data_dir /root/tensorflow_datasets. Using currently defined version 1.0.0.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "15000\n",
            "15000\n",
            "15000\n",
            "15000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYRZGR8CT_r6",
        "colab_type": "text"
      },
      "source": [
        "## Percentage slicing  \n",
        "We can define the percentage of the TRAIN dataset as a subset.\n",
        "We did not use all data from TRIN dataset [30:75]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C2dmPP4UDWp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "c8124b46-f39e-4df6-e3df-dcada31b4063"
      },
      "source": [
        "first_30_percent = tfds.Split.TRAIN.subsplit(tfds.percent[0:30])\n",
        "last_5_percent = tfds.Split.TRAIN.subsplit(tfds.percent[95:])\n",
        "middle_20_percent = tfds.Split.TRAIN.subsplit(tfds.percent[75:95])\n",
        "\n",
        "dataset_split_1 = tfds.load(\"mnist:1.0.0\", split=first_30_percent)\n",
        "dataset_split_2 = tfds.load(\"mnist:1.0.0\", split=last_5_percent)\n",
        "dataset_split_3 = tfds.load(\"mnist:1.0.0\", split=middle_20_percent)\n",
        "\n",
        "print(len(list(dataset_split_1)))\n",
        "print(len(list(dataset_split_2)))\n",
        "print(len(list(dataset_split_3)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found a different version 3.0.0 of dataset mnist in data_dir /root/tensorflow_datasets. Using currently defined version 1.0.0.\n",
            "WARNING:absl:Found a different version 3.0.0 of dataset mnist in data_dir /root/tensorflow_datasets. Using currently defined version 1.0.0.\n",
            "WARNING:absl:Found a different version 3.0.0 of dataset mnist in data_dir /root/tensorflow_datasets. Using currently defined version 1.0.0.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "18000\n",
            "3000\n",
            "12000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si3KuBStVFR1",
        "colab_type": "text"
      },
      "source": [
        "## Weighted splits\n",
        "Devide dataset based on weight parameter.  \n",
        "Weights are numeric components that are scaled to percentage value automatically. sum the weights and devide by specific one.ex:  \n",
        "4/2 = 0.5 == 50%  \n",
        "4/1 = 0.25 == 25%  \n",
        "4/1 = 0.25 == 25%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B260IZ-4VIjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "half, quarter1, quarter2 = tf.Split.TRAIN.subsplit(weighted=[2, 1, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33k5wryAWCn8",
        "colab_type": "text"
      },
      "source": [
        "## Composing perations  \n",
        "Helf of the TRAIN split plus the TEST split  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwV3JH0gWMv7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "outputId": "66a8df6c-ec2e-4c84-e98a-57c2ec0faa4c"
      },
      "source": [
        "first_50_train = tfds.Split.TRAIN.subsplit(tfds.percent[:50])\n",
        "split = first_50_train + tfds.Split.TEST  \n",
        "dataset = tfds.load('mnist:1.0.0', split = split)\n",
        "print(len(list(dataset)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found a different version 3.0.0 of dataset mnist in data_dir /root/tensorflow_datasets. Using currently defined version 1.0.0.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "40000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr5kTPRxW40W",
        "colab_type": "text"
      },
      "source": [
        "## Invalid usages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TksOnh2lW7Qw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Invalid! TRAIN included twice\n",
        "split = tfds.Split.TRAIN.subsplit(tfds.percent[:25])+ tfds.Split.TRAIN\n",
        "\n",
        "# Invalid! Subsplit of subsplit\n",
        "split = tfds.Split.TRAIN.subsplit(tfds.percent[0:25]).subsplit(k=2)\n",
        "\n",
        "# Invalid! TRAIN included twice\n",
        "split = (tfds.Split.TRAIN.subsplit(tfds.percent[:25]) +\n",
        "         tfds.Split.TEST).subsplit(tfds.percent[0:50])\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S93fmGKktOMm"
      },
      "source": [
        "## Using the New S3 API\n",
        "\n",
        "Before using the new S3 API, we must first find out whether the MNIST dataset implements the new S3 API. In the cell below we indicate that we want to use version `3.*.*` of the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4Smp4lYItSSi",
        "outputId": "cef08304-f507-48d0-a88f-84383ac0cbf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "mnist_builder = tfds.builder(\"mnist:3.*.*\")\n",
        "\n",
        "print(mnist_builder.version.implements(tfds.core.Experiment.S3))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55CZC4K_Qydr",
        "colab_type": "text"
      },
      "source": [
        "We can see that the code above printed `True`, which means that version `3.*.*` of the MNIST dataset supports the new S3 API.\n",
        "\n",
        "Now, let's see how we can use the S3 API to download the MNIST dataset and specify the splits we want use. In the code below we download the `train` and `test` splits of the MNIST dataset and then we print their size. We will see that there are 60,000 records in the training set and 10,000 in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fWDJcqHmtpwM",
        "outputId": "62a54354-f98c-4b56-b0ab-4e86ee7b7f90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "train_ds, test_ds = tfds.load('mnist:3.*.*', split=['train', 'test'])\n",
        "\n",
        "print(len(list(train_ds)))\n",
        "print(len(list(test_ds)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6DRu7EGQydx",
        "colab_type": "text"
      },
      "source": [
        "In the S3 API we can use strings to specify the slicing instructions. For example, in the cell below we will merge the training and test sets by passing the string `’train+test'` to the `split` argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4cvBpzUCuVBT",
        "outputId": "b340e7c0-a92a-4f2d-87d9-71790bea278f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "combined = tfds.load('mnist:3.*.*', split='train+test')\n",
        "\n",
        "print(len(list(combined)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "70000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dddc7cEPQyd3",
        "colab_type": "text"
      },
      "source": [
        "We can also use Python style list slicers to specify the data we want. For example, we can specify that we want to take the first 10,000 records of the `train` split with the string `'train[:10000]'`, as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MUUGRKh9uxkG",
        "outputId": "95e97d44-4b4a-40ca-d047-591bd1ce8827",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "first10k = tfds.load('mnist:3.*.*', split='train[:10000]')\n",
        "\n",
        "print(len(list(first10k)))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkp3gcciQyd-",
        "colab_type": "text"
      },
      "source": [
        "The S3 API, also allows us to specify the percentage of the data we want to use. For example, we can select the first 20\\% of the training set with the string `'train[:20%]'`, as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R41Psxi9vn4E",
        "outputId": "e1e9dbc6-a416-4b0d-ad3b-840236e74c62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "first20p = tfds.load('mnist:3.*.*', split='train[:20%]')\n",
        "\n",
        "print(len(list(first20p)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V-TxMC8QyeF",
        "colab_type": "text"
      },
      "source": [
        "We can see that `first20p` contains 12,000 records, which is indeed 20\\% the total number of records in the training set. Recall that the training set contains 60,000 records. \n",
        "\n",
        "Because the slices are string-based we can use loops, like the ones shown below, to slice up the dataset and make some pretty complex splits. For example, the loops below create 10 complimentary validation and training sets (each loop returns a list with 5 data sets)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rnUGAufZv7GL",
        "colab": {}
      },
      "source": [
        "val_ds = tfds.load('mnist:3.*.*', split=['train[{}%:{}%]'.format(k, k+20) for k in range(0, 100, 20)])\n",
        "\n",
        "train_ds = tfds.load('mnist:3.*.*', split=['train[:{}%]+train[{}%:]'.format(k, k+20) for k in range(0, 100, 20)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bbHQuNwQyeJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "26ea4e68-121a-4ed6-86ad-d8479f92848c"
      },
      "source": [
        "val_ds"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<DatasetV1Adapter shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>,\n",
              " <DatasetV1Adapter shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>,\n",
              " <DatasetV1Adapter shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>,\n",
              " <DatasetV1Adapter shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>,\n",
              " <DatasetV1Adapter shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOx8aG-tQyeO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "1c96281d-3558-492f-8345-e7c8b6b94dd6"
      },
      "source": [
        "train_ds"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<DatasetV1Adapter shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>,\n",
              " <DatasetV1Adapter shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>,\n",
              " <DatasetV1Adapter shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>,\n",
              " <DatasetV1Adapter shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>,\n",
              " <DatasetV1Adapter shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffUkzFnMQyeR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "60d023ef-2b96-46db-870a-f736c52227ad"
      },
      "source": [
        "print(len(list(val_ds)))\n",
        "print(len(list(train_ds)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP-TLv5PQyeV",
        "colab_type": "text"
      },
      "source": [
        "The S3 API also allows us to compose new datasets by using pieces from different splits. For example, we can create a new dataset from the first 10\\% of the test set and the last 80\\% of the training set, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SX0wsD9IwPYR",
        "outputId": "4d07793c-d6c9-4ba4-e6e4-b17e5ceaeb51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "composed_ds = tfds.load('mnist:3.*.*', split='test[:10%]+train[-80%:]')\n",
        "\n",
        "print(len(list(composed_ds)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "967y2aFMeat5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}